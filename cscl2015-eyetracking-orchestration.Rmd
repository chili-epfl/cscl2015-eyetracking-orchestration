---
title: "The Burden of Facilitating Collaboration: Towards Estimation of Teacher Orchestration Load using Eye-tracking Measures"
author: "Luis P. Prieto, Kshitij Sharma, Yun Wen & Pierre Dillenbourg"
output: html_document
---

This is an R Markdown document to reproduce the main data preprocessing, analysis and visualization for the homonymous [CSCL2015 conference](http://isls.org/cscl2015/) paper. Below, we reproduce the abstract of the paper, a summary of the context and methods of the studies, and then the analysis and visualization of results from the studies.

## Abstract

Teacher facilitation of CSCL activities is widely recognized as one of the main factors affecting student learning outcomes in formal face-to-face settings. However, the orchestration load that such facilitation represents for the teacher, within the constraints of an authentic classroom, remains under-researched. This paper presents a novel method to estimate the cognitive load of teachers during facilitation of CSCL sessions, using mobile eye-tracking techniques. Throughout **three studies** of increasing authenticity, we demonstrate the feasibility of this approach, and extract insights about classroom usability challenges in CSCL practice: the increased load of class-level facilitation, or the real-time monitoring of students’ progress. This new instrument in the CSCL researcher’s toolkit can help focus our attention in critical, fine-grained classroom usability episodes, to make more informed design decisions.

## The studies: Context and Methods

Our main research question is: _can we use eye-tracking techniques to follow cognitive load of teachers facilitating CSCL in authentic settings?_ In order to explore this question, we set out to apply the measurement of the four metrics used by [Buettner (2013)](http://link.springer.com/chapter/10.1007/978-3-642-40942-4_4) in three studies (with increasing degree of authenticity).

Study | 1 - Analytical | 2 - Semi-authentic | 3 - Authentic
------|------------|----------------|----------
Setting | Laboratory | Multi-tabletop classroom, lab ‘open doors’ day | Authentic course, classroom including projector and laptops
Goal | Test method in different task, see evolution of CL over time | Feasibility of eye-tracking within classroom constraints, insights about multi-tabletop classroom usability | Feasibility of eye-tracking in real course, individual differences of novice/expert teachers
Subjects | 16 participants | 1 facilitator-researcher, 61 primary school students | 1 expert teacher, 1 novice teacher, 12-14 students
Task | Game-based | Facilitation of small group collaborative work | Mix of lecture and collaborative work
Study duration | 128 games in total, 1.5-4 minutes each | 3 sessions, 35-45 min each | 3 sessions (2 expert, 1 novice), 45-65 min each
Analysis | Calculation of Load Index, comparison with Tetris game metrics | Calculation of Load Index, Video coding, Comparison of high- and low-load orchestration profiles | Calculation of Load Index, Video coding, Comparison of high- and low-load orchestration profiles

The data analysis of the three studies relies mainly on the calculation of what we call the **Load Index**, inferred from eye-tracking measures of the subject. The four measures used by Buettner (mean pupil diameter, pupil diameter standard deviation, saccade speed and number of fixations longer than 500ms) were calculated over a sliding window of 10 seconds (with a 5s slide from one window to the next). Then, a median cut was performed (using the median value of that measurement for the session). Finally, the Load Index was calculated by counting the number of measures that were above the game median for that 10-second window (thus going from 0 to 4), as an estimation of how likely it is that a certain 10-second window represented higher cognitive load than other windows in that session.

In the second and third studies, the episodes (10-second windows) of extreme Load Index (0 and 4) were manually **video coded**, to describe different aspects of that episode, with regard to the orchestration taking place in that moment:

Orchestration dimension | Teacher activity | Social plane | Main gaze focus
------------------------|------------------|--------------|----------------
Example codes | Explanation/Lecturing (EXP), Monitoring (MON), Task distribution or transition (TDT), Technical or conceptual repairs (REP)... | Individual (IND), Small group (GRP), Class-wide (CLS) | Students’ faces (FAC) or backs (BAK), Tabletop surface (TAB), Paper worksheet (PAP)...

Then, the high- and low- Load Index episodes are compared along these three dimensions, to see if they are significantly different (with a Pearson's chi-squared test of independence) in terms of how much certains codes appear in episodes with different Load Index.

## Before starting: Data download

First of all, we download the datasets for the three studies, which have been published in Zenodo: 

* [Dataset for Study 1 (Tetris games) MISSING LINK]()
* [Dataset for Study 2 (Multi-tabletop sessions in the lab, primary school students) MISSING LINK]()
* [Dataset for Study 3 (Laptop-and-projector sessions in a university course) MISSING LINK]()

```{r}
# We load the useful scripts and packages needed throughout the report
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")

rootdir <- getwd()
# If not present already, download dataset study 1 and uncompress it to Study1/
setwd(paste(rootdir,"/data/Study1",sep=""))
# TODO: complete once the datasets are uploaded
# if(!file.exists())...
# unzip/bz2...
# If not present already, download dataset study 2 and uncompress it to Study2/
# If not present already, download dataset study 3 and uncompress it to Study3/
```

## Study 1 (Analytical study): cognitive load in a simple game-based task 

As a first step in our exploration of eye-tracking to follow teacher cognitive load, and taking into account that eye responses are often task-dependent, we devised a first _test for the validity of the measurements proposed by Buettner in a totally different task_. Using existing eye-tracking data from a previous experiment in which participants played a computer game (unrelated to the task in Buettner’s study), we estimated the evolution of the CL of participants throughout their experience, to see whether the results were consistent with what we knew about the game task in question and its temporal evolution.

### Data pre-processing

We uncompress the data from the study (warning, it is almost 5G of files!). This includes mainly a time series of eyetracking data along with the value of several game variables (e.g., mean Tetris stack height) in each instant, plus an additional set of files with the fixation details of each game in the experiment. 

Once we uncompress the data, we run the pre-processing of data: basically, separating the data for each game, and calculating the four eyetracking variables of interest, over 10-second rolling windows with 5-second slide:

* Pupil diameter mean
* Pupil diameter standard deviation
* Number of fixations with duration >500ms
* Average saccade speed

(see ```./data/Study1/preprocessStudy1Data.R``` and ```./lib/rollingWindows.R``` files for details)

```{r, cache=TRUE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data/Study1",sep=""))
source("preprocessStudy1Data.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file
cleandatafile <- "Study1ProcessedData.Rda"

# TODO: delete this, the uncompression is done at the top
if(!file.exists(cleandatafile)){
    if(!file.exists("ALLCombinedVariables_timeseriesPupilEvolution.csv")){
        unzip("ALLCombinedVariables_timeseriesPupilEvolution.csv.zip")                
    }
    unzip("FixationDetails.zip")
    preprocessStudy1()
}
```

### Data analysis

The main basic data analysis we do here is just to calculate the Load Index for each 10s window in a game (see ```./lib/loadIndex.R``` file for details):

```{r}
# We load the overall dataset with the data from the 10s sliding windows
totaldata <- get(load(paste(rootdir,"/data/Study1/",cleandatafile,sep="")))

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each game a different session
# for the effects of the median cut
games = unique(totaldata$gameID[])
for(game in games){
    data <- totaldata[totaldata$gameID == game,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data)    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized

```


### Main Results and visualization

If we look at the mean values of Load index and different game variables (mainly, the Tetris mean stack height and its variance)

**1. Temporal evolution**

```{r, fig.height=4, fig.width=7, message=FALSE, warning=FALSE}
require("gplots")
require("ggplot2")
require("reshape2")

# We can plot the Load Index graph of one such game, to understand how it looks like
gamedata <- loaddata[loaddata$gameID == games[1],]
ggplot(gamedata, aes(x=time, y=Load, col=Load)) + 
            ggtitle(paste("Load Index, single game\n(estimation of cognitive overload over 10s)",sep="")) + 
            geom_line(size=1) +
            theme(axis.text.x = element_blank(),plot.title=element_text(size=20, face="bold"),axis.title=element_text(size=18),panel.background = element_rect(fill = 'white')) +
            scale_color_gradient(low="green",high="red")

```

We can see that for a single game the load index is quite noise, but has a certain descending trend over time. It may be more interesting to look at the aggregates of all the games we recorded, to see the average evolution of Load Index over time, and also the evolution of the average main game metrics over time:

```{r, fig.height=4, fig.width=7, message=F, warning=F}
# We select the main variables of interest: time, Load, value.stackMean, value.stackVar
datagraph <- loaddata[,c("time","Load","value.stackMean","value.stackVar")]
# We square the load, to more easily compare it with the game variables' scale
datagraph$Load<-(datagraph$Load)^2
colnames(datagraph) <- c("time","(load index)^2","stack height mean","stack height variance")
melteddatagraph <- melt(datagraph,id="time")
ggplot(melteddatagraph,aes(x=time,y=value,colour=variable))+stat_smooth()+ylim(c(0,25))
```

We can see the average cognitive load index (brown curve) of participants as time went on, as well as the temporal evolution of the stack height (green curve) and stack variance (blue curve). If we think in terms of the particular task (the Tetris game), we get interesting insights into how the cognitive load may evolve over time: at the beginning (low mean stack heights) the cognitive load is high (as many alternative options for placing a new piece are open to decide amongst), and it generally goes down as the game goes towards the end (higher mean stack height), until we eventually disengage from the game when we give up. Similar but opposite is the effect of stack variance (higher variance implies more complex stack profiles, difficult to process and with more open alternatives for placement of the next block).

**2. Load Index vs. Game variables (stack height mean/variance)**

```{r, message=FALSE, warning=FALSE}
plotmeans(loaddata$value.stackMean~loaddata$Load, 
          xlab="Load Index",
            ylab="Mean stack height (avg. 10s episode)",
            main="Load Index vs. Stack height", barwidth=2)
plotmeans(loaddata$value.stackVar~loaddata$Load,
          xlab="Load Index",
            ylab="Stack height variance (avg. 10s episode)",
            main="Load Index vs. Stack height", barwidth=2)
```

The two figures showing main descriptive statistics of both game variables on 10-second windows (classified by their load index of each episode), show that the Load Index is positively correlated with stack variance, and negatively correlated with the mean stack height, and that such an effect is more clearly apparent in the extreme values of the load index.

### Summary of results

These results show that the Load Index, computed as described above, has potential for distinguishing different kinds of episodes occurring during the task (represented by moments with different mean stack heights and variances). We also see how CL may be related with the amount of open alternatives in each moment (in a sense, the uncertainty or the ‘entropy’ we perceive about the current game situation).

## Study 2 (Semi-authentic study): multi-tabletops at an open-doors day in the lab

For the next study we aimed to explore the following research questions: _is it feasible to use a mobile eye-tracker to follow CL in a semi-authentic classroom setting? Does such analysis provide interesting insights about classroom usability of a novel CSCL technology? Can we detect specific classroom interaction episodes that imply high (or low) cognitive load?_.

### Data pre-processing

If needed, we uncompress the data from the study. This includes mainly a time series of eyetracking data for each of the three sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons.

Once we uncompress the data, we run the pre-processing of data,  calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide, and then merge that data with the video coding data (see ```./data/Study2/preprocessStudy2Data.R``` and ```./lib/rollingWindows.R``` files for details).

```{r, cache=FALSE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data/Study2",sep=""))
source("preprocessStudy2Data.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file

cleandatafile <- "Study2ProcessedData.Rda"
if(!file.exists(cleandatafile)){
    sessions <- c("JDC2014-Session1-eyetracking","JDC2014-Session2-eyetracking","JDC2014-Session3-eyetracking")
    
    # TODO: delete this, the uncompression is done at the top
    uncompressneeded <- FALSE    
    for (session in sessions){
        #We check whether we have all the raw data elements, uncompressed
        filename1 <- paste(session,"-eventexport.txt",sep="")
        filename2 <- paste(session,"-fixationDetails.txt",sep="")
        filename3 <- paste(session,"-saccadeDetails.txt",sep="")
        if(!file.exists(filename1) || !file.exists(filename2) || !file.exists(filename3)) uncompressneeded <- TRUE
    }
    
    if(uncompressneeded) unzip("JDC2014-EyetrackingData.zip")                
    data <- preprocessStudy2()
}

```


### Data analysis
The main basic data analysis we do here is just to calculate the Load Index for each 10s window in a game (see ```./lib/loadIndex.R``` file for details):

```{r, warning=FALSE}
# We load the overall dataset with the data from the 10s sliding windows
totaldata <- get(load(paste(rootdir,"/data/Study2/",cleandatafile,sep="")))
sessions <- c("JDC2014-Session1-eyetracking","JDC2014-Session2-eyetracking","JDC2014-Session3-eyetracking")

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each session separately for the median cut
for(session in sessions){
    newdata <- data.frame()
    data <- totaldata[totaldata$Session == session,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data, meanlabel="value.Mean", sdlabel="value.SD", fixlabel="value.Fix", saclabel="value.Sac")    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized
```

Once we have this load index, the extreme load moments (those with Load Index equal to 0 or 4) are extracted in order to be video coded by a researcher, using the video feed from the eyetracker. The code to generate the csv file to be filled in by the coder is in the ```./lib/extremeLoadExtraction.R``` file:

```{r, warning=FALSE}
outputfile <- paste(rootdir,"/data/Study2/TimesToVideoCode.csv",sep="")
extremedata <- extractExtremeLoadMoments(sessions, loaddata, outputfile)
```

Once the video files are coded, the output should be in a csv file such as the one provided in our dataset. We then match those video codes with our load index data coming from the eyetracking:

```{r, warning=FALSE}
videosessions <- c("JDC2014-Session1-videocoding","JDC2014-Session2-videocoding","JDC2014-Session3-videocoding")

videodata <- data.frame()
for(videosession in videosessions){
    data <- read.csv(paste(rootdir,"/data/Study2/",videosession,".csv",sep=""))
    if(nrow(videodata)==0) videodata <- data   
    else videodata <- rbind(videodata,data)
}

videocodeddata <- merge(loaddata,videodata,by = c("Session","time"), all = T)

# We ensure that the load and video coded variables are factors, for later analysis, and eliminate any levels of the factor not used,
# and keep only the extreme load cases (in the dataset there are codes for some non-extreme values too)
videocodeddata <- videocodeddata[complete.cases(videocodeddata) & (videocodeddata$Load == 4 | videocodeddata$Load == 0),]
videocodeddata$Load <- as.factor(videocodeddata$Load)
videocodeddata$Activity <- factor(videocodeddata$Activity)
videocodeddata$Social <- factor(videocodeddata$Social)
videocodeddata$Focus <- factor(videocodeddata$Focus)
```

### Main results

In this study, we mainly want to see if the high- and low-load index episodes are significatively different, in terms of the video codes assigned to them (the teacher's activity, the social plane of interaction for that activity, and the main gaze focus).

#### Teacher activity

```{r, warning=FALSE}
tabAct <- table(videocodeddata$Load,videocodeddata$Activity)
tabAct
```

We can see the activity profiles for high- and low-load episodes are different but... how different? We perform a chi-squared test, and look at the residuals to see which video codes contribute significantly to these differences (e.g., which residuals are greater than 1.96):

```{r, warning=FALSE}
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see some trends in the high- and low-load episodes (low-load tend to be more monitoring and repair episodes; high-load tend to be more explanations and task transitions), although only the contribution of the TDT (task transition and distribution) to high-load episodes is significant.

#### Social plane of interaction

```{r, warning=FALSE}
tabSoc <- table(videocodeddata$Load,videocodeddata$Social)
tabSoc
```

We can see the social plane profiles for high- and low-load episodes are totally opposed in this case. We perform a chi-squared test, and look at the residuals to see which video codes contribute significantly to these differences (e.g., which residuals are greater than 1.96):

```{r, warning=FALSE}
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see that all video codes contribute significantly to these differences, meaning that _high-load episodes tend to be more often in the whole-class plane_, while _low-load episodes lie almost exclusively in the small-group plane_.


#### Main focus of teacher gaze

```{r, warning=FALSE}
tabFoc <- table(videocodeddata$Load,videocodeddata$Focus)
tabFoc
```

We can see the main focus profiles for high- and low-load episodes are also quite different. We perform a chi-squared test, and look at the residuals to see which video codes contribute significantly to these differences (e.g., which residuals are greater than 1.96):

```{r, warning=FALSE}
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see that all video codes contribute significantly to these differences, meaning that _high-load episodes tend to be more often in the whole-class plane_, while _low-load episodes lie almost exclusively in the small-group plane_.


### Summary of results

A statistical analysis (Pearson’s chi-squared test of independence) of the video coding for the episodes with minimum and maximum load index revealed that high-load and low-load episodes had statistically significant differentiated profiles in all three coding dimensions: teacher activity, activity's social plane and the main focus of teacher's gaze. By looking at the contributions of the different video codes to this statistical difference (the chi-squared test residuals for each code), we could identify the distinct profiles of high- and low-load episodes (i.e., what where the video codes appearing typically in one case or the other): high-load episodes had a higher chance of being transition/task distribution activities, and to occur at the classroom-level, featuring the students’ faces (e.g., when explaining) or backs (e.g., when monitoring the progress of activities), or the teacher’s own desk, which was cluttered with multiple paper elements to be distributed to the student groups as they progressed along the lesson activities. In contrast, low-load episodes occurred almost exclusively at the group social plane, while the teacher was focusing exclusively on one student tabletop.

The results of this study illustrate that it is feasible to use eye-tracking in a semi-authentic CSCL situation (e.g., the calibration procedure needed at the beginning of the eye-tracker recording could easily fit at the beginning of the lesson), and that the load index calculated using such techniques can be used to distinguish high/low load episodes. Furthermore, the study provided certain insights into classroom usability aspects of the specific CSCL technology used in this classroom: the need for classroom-level monitoring support in multi-tabletop classrooms and the dangers of a cluttered augmented paper user interface.





## Study 3 (Authentic study): master-level university course

For the last study, we set out to explore the following research questions: _is it feasible to use mobile eye-tracker to follow cognitive load in an authentic classroom/lesson?_ But also, since eye movement patterns can vary greatly from person to person and cognitive overload is known to be related to teaching expertise, we aimed at exploring a second question: _do teachers with different teaching experience show different load episode patterns?_.

### Data pre-processing

If needed, we uncompress the data from the study. This includes mainly a time series of eyetracking data for each of the three sessions, plus an additional set of files with the fixation details and saccade details of each session. There is also another file with the codes assigned to a fraction of the of 10-second episodes (those with extreme load indices), as coded by a single human researcher, following the three-dimension video coding scheme mentioned above. The raw gaze video data itself has not been made available due to anonymity reasons.

Once we uncompress the data, we run the pre-processing of data,  calculating the four eyetracking variables of interest (see previous section), over 10-second rolling windows with 5-second slide, and then merge that data with the video coding data (see ```./data/Study3/preprocessStudy3Data.R``` and ```./lib/rollingWindows.R``` files for details).

```{r, cache=FALSE, message=FALSE, warning=FALSE}
setwd(paste(rootdir,"/data/Study3",sep=""))
source("preprocessStudy3Data.R")
# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file

cleandatafile <- "Study3ProcessedData.Rda"
if(!file.exists(cleandatafile)){
    sessions <-  c("DELANA-Session1-Expert-eyetracking","DELANA-Session2-Expert-eyetracking","DELANA-Session3-Novice-eyetracking")
    
    # TODO: delete this, the uncompression is done at the top
    uncompressneeded <- FALSE    
    for (session in sessions){
        #We check whether we have all the raw data elements, uncompressed
        filename1 <- paste(session,"-eventexport.txt",sep="")
        filename2 <- paste(session,"-fixationDetails.txt",sep="")
        filename3 <- paste(session,"-saccadeDetails.txt",sep="")
        if(!file.exists(filename1) || !file.exists(filename2) || !file.exists(filename3)) uncompressneeded <- TRUE
    }
    
    if(uncompressneeded) unzip("DELANA-EyetrackingData.zip")                
    data <- preprocessStudy3()
}

```


### Data analysis
The main basic data analysis we do here is just to calculate the Load Index for each 10s window in a game (see ```./lib/loadIndex.R``` file for details):

```{r, warning=FALSE}
# We load the overall dataset with the data from the 10s sliding windows
totaldata <- get(load(paste(rootdir,"/data/Study3/",cleandatafile,sep="")))
sessions <-  c("DELANA-Session1-Expert-eyetracking","DELANA-Session2-Expert-eyetracking","DELANA-Session3-Novice-eyetracking")

# This dataframe will contain the data with the added Load Index data
loaddata <- data.frame()

# We calculate the load index, considering each session separately for the median cut
for(session in sessions){
    newdata <- data.frame()
    data <- totaldata[totaldata$Session == session,]
    # We add the columns with Load Index data
    newdata <- calculateLoadIndexSession(data, meanlabel="value.Mean", sdlabel="value.SD", fixlabel="value.Fix", saclabel="value.Sac")    
    # We join the new data into a dataset with all session's data
    if(length(loaddata)==0) loaddata <- newdata
    else loaddata <- rbind(loaddata,newdata)
}       

# Now, loaddata will contain the data to be summarized/visualized
```

Once we have this load index, the extreme load moments (those with Load Index equal to 0 or 4) are extracted in order to be video coded by a researcher, using the video feed from the eyetracker. The code to generate the csv file to be filled in by the coder is in the ```./lib/extremeLoadExtraction.R``` file:

```{r, warning=FALSE}
outputfile <- paste(rootdir,"/data/Study3/TimesToVideoCode.csv",sep="")
extremedata <- extractExtremeLoadMoments(sessions, loaddata, outputfile)
```

Once the video files are coded, the output should be in a csv file such as the one provided in our dataset. We then match those video codes with our load index data coming from the eyetracking:

```{r, warning=FALSE}
videodata <- read.csv(paste(rootdir,"/data/Study3/DELANA-videocoding.csv",sep=""))

videocodeddata <- merge(loaddata,videodata,by = c("Session","time"), all = T)

# We ensure that the load and video coded variables are factors, for later analysis, and eliminate any levels of the factor not used,
# and keep only the extreme load cases (in the dataset there are codes for some non-extreme values too)
videocodeddata <- videocodeddata[complete.cases(videocodeddata) & (videocodeddata$Load == 4 | videocodeddata$Load == 0),]
videocodeddata$Load <- as.factor(videocodeddata$Load)
videocodeddata$Activity <- factor(videocodeddata$Activity)
videocodeddata$Social <- factor(videocodeddata$Social)
videocodeddata$Focus <- factor(videocodeddata$Focus)
```


### Main results

In this study, we again want to see if, overall, the high- and low-load index episodes are significatively different, in terms of the video codes assigned to them (the teacher's activity, the social plane of interaction for that activity, and the main gaze focus). However, we also want to see if there are differences in the trends for the novice and the expert teachers that were recorded.

#### Overall load profiles and trends

We perform a similar video code count and statistical tests, as in the previous study. First, we look at the **teacher activity**:

```{r, warning=FALSE}
# For the teacher activity
tabAct <- table(videocodeddata$Load,videocodeddata$Activity)
tabAct
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

We can see the activity profiles for high- and low-load episodes are somehow different, but the difference is _not statistically significant_.

If we look at the **social plane** of interaction:

```{r, warning=FALSE}
# We merge individual and group social planes, as they are largely equivalent in the context of these sessions
levels(videocodeddata$Social) = c("CLS","GRPIND","GRPIND")
tabSoc <- table(videocodeddata$Load,videocodeddata$Social)
tabSoc
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and although none of the codes is significant by itself, we can see that there is again a tendency of high-load episodes to be whole-class, while low-load episodes have a mix of individual or small-group plane, but also a large quantity of whole-class episodes among them.

Finally, we can look at the **main gaze focus**:

```{r, warning=FALSE}
tabFoc <- table(videocodeddata$Load,videocodeddata$Focus)
tabFoc
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and we can see that there are several trends contributing to this difference, the strongest of which is the unlikeliness of having a high-load episode focused on the teacher's computer (TCOMP). Less significant trends include the focus on the teacher's own table in low-load episodes, or the focus on students' faces or the class's whiteboard during high-load episodes.


#### Per-participant load profiles and trends

As we see, the trends in this study are not as clear as in the previous one. Taking into account that this study includes the data of a novice and an expert teachers, we can try to analyze if the different teachers have different load episode patterns, even if the classroom dynamic was similar.

##### Novice teacher

We filter for the session recorded by the novice teacher, and perform a similar video code count and statistical tests. First, we look at the **teacher activity**:

```{r, warning=FALSE}
partdata <- videocodeddata[grepl("Novice",videocodeddata$Session),]

# We eliminate the video codes that do not appear for this participant
partdata$Activity <- factor(partdata$Activity)
# For the teacher activity
tabAct <- table(partdata$Load,partdata$Activity)
tabAct
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

We can see the activity profiles for high- and low-load episodes _are statistically significantly different_. Several trends seem to be contrib uting to this significance, especially explanation activities to high-load episodes, and solving problems or questions (i.e., repairs, REP) to low-load episodes.

If we look at the **social plane** of interaction:

```{r, warning=FALSE}
# We merge individual and group social planes, as they are largely equivalent in the context of these sessions
levels(partdata$Social) = c("CLS","GRPIND","GRPIND")
tabSoc <- table(partdata$Load,partdata$Social)
tabSoc
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

Thus, we can see that the differences _are statistically significant_, and although none of the codes is significant by itself, we can see that there is again a tendency of high-load episodes to be whole-class, while low-load episodes have a mix of individual or small-group plane, but also an appreciable quantity of whole-class episodes among them.

Finally, we can look at the **main gaze focus**:

```{r, warning=FALSE}
# We eliminate the video codes that do not appear for this participant
partdata$Focus <- factor(partdata$Focus)
# For the main focus of gaze
tabFoc <- table(partdata$Load,partdata$Focus)
tabFoc
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

Thus, we can see that the differences _are statistically significant_ (although barely so), and we can see that there are several trends contributing to this difference, such as the focus on the teacher's computer (TCOMP) in low-load episodes, or the focus on the faces of students (FAC) or the classroom's projector (PROJ) in the high-load episodes.


##### Expert teacher

We filter for the session recorded by the expert teacher, and perform a similar video code count and statistical tests. First, we look at the **teacher activity**:

```{r, warning=FALSE}
partdata <- videocodeddata[grepl("Expert",videocodeddata$Session),]

# We eliminate the video codes that do not appear for this participant
partdata$Activity <- factor(partdata$Activity)
# For the teacher activity
tabAct <- table(partdata$Load,partdata$Activity)
tabAct
chisq.test(tabAct)
chisq.test(tabAct)$residuals
```

We can see the activity profiles for high- and low-load episodes _not statistically significantly different_ in this case, and trends in this case are much less clear than in the previous ones.

If we look at the **social plane** of interaction:

```{r, warning=FALSE}
# We merge individual and group social planes, as they are largely equivalent in the context of these sessions
levels(partdata$Social) = c("CLS","GRPIND","GRPIND")
tabSoc <- table(partdata$Load,partdata$Social)
tabSoc
chisq.test(tabSoc)
chisq.test(tabSoc)$residuals
```

We can see that the differences are again _not statistically significant_, although we can see similar trends (e.g., it is relatively unlikely to have high-load episodes in a group-individual plane of interaction), only much weaker.

Finally, we can look at the **main gaze focus**:

```{r, warning=FALSE}
# We eliminate the video codes that do not appear for this participant
partdata$Focus <- factor(partdata$Focus)
# For the main focus of gaze
tabFoc <- table(partdata$Load,partdata$Focus)
tabFoc
chisq.test(tabFoc)
chisq.test(tabFoc)$residuals
```

In this case we see differences in high- and low-load episode profiles _are statistically significant_. Several trends are contributing to this difference, such as the focus on the teacher's computer (TCOMP) or the classroom's projector (PROJ) in low-load episodes, or the focus on the faces or backs of students (FAC, BAK) or the classroom's whiteboard (WHIT) in the high-load episodes.


### Summary of results

The results of this third study confirm some of the findings of the previous studies in this series (the increased load of classroom-level activities, or in trying to read students’ faces, probably to assess their understanding). They also served to spot cognitive load pattern differences between an expert and a novice teacher (novice’s high CL episodes being more concentrated in distinct kinds of activities/focus than the expert’s -- probably because the expert teacher was never really overloaded, and thus the Load Index will be a much noisier source of data). Finally, this study further confirms that this method for following CL of a teacher/facilitator is usable in authentic situations (although it requires the presence of a researcher/assistant for calibrating the device).

## Conclusions

Overall, the three aforementioned studies show that the proposed combination of mobile eye-tracking measurements can be feasibly used in authentic classroom settings. Together with post-hoc qualitative video coding, the approach also showed potential for discriminating fine-grained critical episodes (either high- or low-load) during the CSCL enactment, without having to rely on the teacher's memory of the events. The distinct profiles of such episodes have helped us gain insights into the difficulties of orchestrating CSCL activities: the need for awareness/monitoring support in multi-tabletop classrooms, and the importance of classroom-level awareness in general, the challenge that clutter and a scattered user interface pose in augmented paper applications. The results also show that the facilitation CL is highly dependent on the teacher's prior experience.

Our results across the three studies also provide a new insight about orchestration load, which should be explored in further studies: the fact that load seems to be correlated to the amount of "open alternatives" (i.e., the perceived uncertainty/entropy of the classroom situation). More novice teachers seemed to be especially sensitive to these high-uncertainty situations (e.g., looking at students faces during an explanation, trying to assess their comprehension; looking at students' backs while working in the tabletops, trying to assess their progress). This need of novice teachers for classroom management support can be used as a starting point for new technologies that can ameliorate the challenge of this kind of episodes (e.g., a system that helps novice lecturers to assess the attention or comprehension of their students).
